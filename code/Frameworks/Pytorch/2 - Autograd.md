you use this in order to calculate gradients 
this will calculate all gradients for you

1. x = torch.randn(3, requires_grad=True)
2. print(x)
3. x + 2 = y
4. for each operation we have a node with inputs and outputs -- inputs = x & 2 
5. output = y
6. through back propagation we calc the y and then pytorch calculates a gradient by making a function, with this function we can calculate the  back pass 
7. this allows us to calculate y with respect to x 
8. so like y = x + 2
9. rather than x +2 = y
basically generates a function that is the reverse of the operation you just did with the tensor

z = y * y * 2 
generates a mulbackward function 


 z = z.mean == meanbackward

z.backward -- this calulates the gradient of z in respect to x 


each tensor has an attribute .grad which is the gradient from the function generated by pytorch
to use the backward function you must pass a vector of the same size into the function it will give you a vector product grad

if not a scalar value you must give a vector args

.requires_grad_(Can be set to true or false based on additions later in the code )
or with torch.no_grad()

calling the detach function generates a tensor with the same vlaues or basically a vector because it does not require the gradient

num = 10  # total number of epochs to train
	using epoch in such form :  
	for epoch in range(num):
		print(f"Epoch{epoch + 1}/ {num})
			training code here i.e. forward pass backward pass
epoch means one full cyccle where the model sees the entire training dataset this can occasionally be split up into multiple iterations if the data is set up in small groups

